Support Vector Machine

. cost function similar to logistic regression but simplified
	-- not logarithmic anymore
	cost_1
	cost_0

... and, get rid of 1/m terms, and use C on outside instead of lambda:
. C * [sum(y^i*cost_1(Theta*x^i) + (1-y)*cost_0(Theta*x^i))] + 1/2*sum(Theta_j^2)
	-- it will give same Theta as equation using lambda/2m if:
			 C = 1/lambda
. practically speaking, C "controls penalty	for misclassified training examples"
	-- high values for C tell SVM to "try to classify all the examples correctly"

Hypothesis doesn't yield a probability, it predicts a 0 or 1 directly


Support Vector Machines, aka "Large Margin Classifiers"
. based on simplified cost function graph: 
	if y=1, we want Theta*X >=1, not just >= 0
	if y=0, we want Theta*X <= -1, not just <= 0

Vector Inner Product
. given 2 2-dimensional vectors u and v:
	p = length of projection of v onto u
	u'v = p * || u ||
		= u1v1 + u2v2
|| u || = length of vector u -- if 2-dimensional, use pythagorean theorem to get it
	== square root(u1^2 + u2^2)

Kernels
. put three "landmark" points on a graph of x1 and x2
. f1 = "similarity" between x and l^1 = 
	*** f = exp(- (|| x - l^1 || ^2) / sigma^2) ***
	exp(x) = e^x 
. f2 = "similarity" between x and l^2
	etc....
. f functions are "kernel" functions
. in particular, "gaussian" kernel functions
	k(x,l^i)
. if numerator is small, exp^0 = 1

Kernels in practice
. for each training example, you develop a feature vector f, with length of m, with each
item corresponding to one of the training examples, where each item is the similarity
between the training example and all the other training examples
. y=1 if Theta'*f >= 0
. cost function becomes:
	C * sum( y^i * cost_1(Theta'*f^i) + (1-y^i)*cost_0(Theta'*f^i)) + 1/2*sum(Theta^2)
	-- in reality, the regularization term is different, but it doesn't matter (?)

Effects of C:
	Large C: leads to lower bias, high variance -- equivalent to small lambda
	Small C: leads to higher bias, low variance -- equivalent to large lambda

lower-case sigma^2: 
	Large: features vary more smoothly -- leads to higher bias, lower variance
	Small: features vary less smoothly -- leads to lower bias, higher variance


Stuff to consider when implementing SVM algorithm
. software does all the heavy lifting
. kernel:
	-- no kernel: "linear kernel"
	-- gaussian kernel
		-- need to choose sigma^2
		-- choose if n is small and/or m is large
		-- perform feature scaling before using
. feature scaling is VERY important, as the final equation is just additive:
	Theta0*f0 + Theta1*f1 ....
. Other types of kernels (but rarely used):
	-- polynomial kernel -- eg, k(x,l) = (xl)^2
	-- string kernel (for strings), chi-square kernel, histogram intersection kernel

Multi-class Classification
. most packages have built-in multi-class classification
. else, do one vs-all k times for each class

Logistic regression vs SVMS
. if n is large relative to m, use logistic regression or SVM without a kernel ("linear kernel")
	e.g., n>=m, n=10,000, m= 10-1000
. if n is small, m is intermediate, use SVM with Guassian kernel
	e.g., n=1-1000, m = 10-1000
. if n is small, m is large:
	create/add more features, then use logistic regression or SVM without a kernel
. neural networks likely to work well for all of these, but likely slower to train


