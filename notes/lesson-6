improving your model

things you can do:
. get more training examples
. use fewer features
. use more features
. add polynomial features
. decrease lambda -- regularization parameter
. increase lambda

__evaluating a hypothesis__

does your model overfit the data?
. split your data:
	-- 70 % training set
	-- 30 % test set
. compute test set error
	-- cost function
	-- misclassification error
		err(h(x),y) = { 1 if h(x) >= .5 and y = 0, or if h(x) < .5 and y = 1 }
			-- in other words, binary 0/1 error score if the prediction is wrong;
				calculate the test error:
		test error = t/m_test * sigma( err(h0(x_test^i),y_test^i) )

__model selection problems__
what features to include, how to set lambda, etc.

adding polynomial terms
** don't get why he introduces cross-validation step here -- why not
** just redo entire model against training set and add polynomial terms then?
	-- in other words, why come up with a model, and once you have it,
		only then start adding polynomial terms?
. instead of just a training and test set:
	-- training (60% of your data), cross-validation (20%), test set (20%)
	-- idea is that you train initial model on training set
	-- then test with polynomial terms on CV set
	-- then do final test on test set

high bias and high variance problems -- underfitting and overfitting problems
. regularization is one way to find the happy medium
	-- find your initial model without regularization
	-- test different values of lambda against it with cross-training set
	-- evaluate on test set


Learning Curves
. plot J_train(theta) or J_cv(theta) as function of m
. average J_train error increases as sample size increases, which makes sense
. average J_cv error should decrease as sample size increases

high bias
. if a learning algorithm has high bias, ie the model doesn't fit the data well,
	getting more training data won't help
	-- you should use j_train and j_cv converge at a high level
high variance
. plotting J_train against sample size again -- you should see j_train slightly
	increase as m increases, which makes intuitive sense
. if, when plotting J_train and J_cv against the sample size, you see a consistently large
gap between the the J_train line and the J_cv line, it is an indication that 
you _variance_ is high
	-- increasing the amount of training data should help

Getting more training examples:
	-- fixes high variance

Smaller sets of features
	-- fixes high variance

Getting additional features
	-- fixes high bias

Adding polynomial features
	-- fixes high bias

Decreasing lambda
	-- fixes high bias

Increasing lambda
	-- fixes high variance

Applied to neural networks:
. can start with a small neural network, few units, few or one hidden layers
. often, however, starting with a large network and addressing overfitting with
	regularization is more effective than using a small network


