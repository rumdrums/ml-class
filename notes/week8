Unsupervised Learning

K-means clustering algorithm
. randomly initialize "centroids"
. iterative:
	1) cluster assignment step -- assign each data point
		to centroid it's closest to
		-- for each i in 1 to m:
				c^i := index of cluster centroid closest to x^i
	2) "inner loop": move centroid step
		-- move them to average of points in that cluster
		-- for each k in 1 to K:
				u_k := average of points assigned to cluster k
	... REPEAT
. inputs:
	-- K: number of clusters
	-- Training set
. K-means for non-well-separated clusters

Optimization objective

. Cost function:
	J(c^1,...,c^m,u_1,...,U_k) = 1/m*sum(||x^i-u_c^i||^2)
		-- find c^i and u_c^i to minimize cost or "distortion"

Random initialization
. K < m --- duh
. randomly pick K training examples
. *** random initialization can lead to different results based on your choices
	-- susceptible to local optima of the cost function
	-- so, loop it a certain number times with different random initializations
		and pick the one that gives lowest cost
. looping the random initializations is more important when k is small
	-- eg, 2 to 10
. large k is less susceptible to problems of random initializations

Choosing number of clusters
. What is right value of K?
	-- Unfortunately, still mostly a manual activity
. Elbow method
	-- look at values of cost function as you increase number of clusters
	-- hopefully the line of cost function plotted against number of clusters
		drops quickly then levels off -- sort of L-shaped
	-- con: the graph is often ambiguous and doesn't look like an L or elbow at all
. "Downstream" considerations
	-- less concern about the data itself and more concerned about what best
		serves your purposes -- ie, if you're a t-shirt manufacturer trying to
		determine how many/what sizes to make

Dimensionality Reduction
. turning mutliple highly correlated features into fewer or a single feature
. dimensionality can help you visualize data -- eg reducing all your features down
	to only 2 or 3 to allow you to view it on a plot

Principal Component Analysis
. if reducing from 2 dimensions to 1:
	goal is to "find direction onto which to 
	project data so as to minimize the projection error"
. in general:
	reduce from n dimensions to ki dimensions -- 
	find k vectors "onto which to project data so 
	as to minimize the projection error"
. PCA is NOT linear regression -- though it looks similar
	-- most immediate difference is that the lines calculated between fit line
		and points are perpendicular -- ie, the shortest distance to the line

Prior to dimensionality reduction
. Feature scaling
. Mean normalization -- zero mean
	(x_j - u_j) / s_j -- s_j usually standard deviation

PCA Algorithm
. Compute covariance matrix -- also represented as sigma, which is lame:
	Sigma = 1/m * Sum( x^i * x^i' ) (each row of x times itself transposed)
		-- VECTORIZED: 1/m ( X' * X )
		-- and x^i is Mean Normalized, as indicated above
	-- returns nXn matrix (results from nXm * m*n multiplication ]

. Compute "eigenvectors" of matrix Sigma:
	-- "svd" and "eig" are two Octave functions that do this
		-- e.g., [U, S, V] = svd(Sigma)
	-- "SVD" = Singular Value Decomposition
	-- U matrix is also nXn -- columns are u vectors to u^m
	-- reduce to K dimensions by taking first K column vectors of U
	-- U_reduce is U with only the k dimensions you want -- so, nXk matrix
	-- Z = U_reduce' * X
		--  nXk' * nXm = 
			kXn * nXm = 
			kXm vector 

Reconstruction after data after compression
. X_approx = U_reduce * Z
	-- nXk * kXm =
	   nXm matrix

