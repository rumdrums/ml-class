Unsupervised Learning

K-means clustering algorithm
. randomly initialize "centroids"
. iterative:
	1) cluster assignment step -- assign each data point
		to centroid it's closest to
		-- for each i in 1 to m:
				c^i := index of cluster centroid closest to x^i
	2) "inner loop": move centroid step
		-- move them to average of points in that cluster
		-- for each k in 1 to K:
				u_k := average of points assigned to cluster k
	... REPEAT
. inputs:
	-- K: number of clusters
	-- Training set
. K-means for non-well-separated clusters

Optimization objective
. Cost/Distortion function:
	J(c^1,...,c^m,u_1,...,U_k) = 1/m*sum(||x^i-u_c^i||^2)
		-- find c^i and u_c^i to minimize cost or "distortion"

Random initialization
. K < m --- duh
. randomly pick K training examples
. *** random initialization can lead to different results based on your choices
	-- susceptible to local optima of the cost function
. How to determine which intiailization to use:
	-- loop it a certain number times with different random initializations
		and pick the one that gives lowest cost
. looping the random initializations is more important when k is small
	-- eg, 2 to 10
. large k is less susceptible to problems of random initializations

Choosing number of clusters
. What is right value of K?
	-- Unfortunately, still mostly a manual activity
. Elbow method
	-- look at values of cost function as you increase number of clusters
	-- hopefully the line of cost function plotted against number of clusters
		drops quickly then levels off -- sort of L-shaped
	-- con: the graph is often ambiguous and doesn't look like an L or elbow at all
. "Downstream" considerations
	-- less concern about the data itself and more concerned about what best
		serves your purposes -- ie, if you're a t-shirt manufacturer trying to
		determine how many/what sizes to make

Dimensionality Reduction
. turning mutliple highly correlated features into fewer or a single feature
. dimensionality can help you visualize data -- eg reducing all your features down
	to only 2 or 3 to allow you to view it on a plot

Principal Component Analysis
. if reducing from 2 dimensions to 1:
	goal is to "find direction onto which to 
	project data so as to minimize the projection error"
. in general:
	reduce from n dimensions to ki dimensions -- 
	find k vectors "onto which to project data so 
	as to minimize the projection error"
. PCA is NOT linear regression -- though it looks similar
	-- most immediate difference is that the lines calculated between fit line
		and points are perpendicular -- ie, the shortest distance to the line

Prior to dimensionality reduction
. Feature scaling
. Mean normalization -- zero mean
	(x_j - u_j) / s_j -- s_j usually standard deviation

PCA Algorithm
. Compute covariance matrix -- also represented as sigma, which is lame:
	Sigma = 1/m * Sum( x^i * x^i' ) (each row of x times itself transposed)
		-- VECTORIZED: 1/m ( X' * X )
		-- and x^i is Mean Normalized, as indicated above
	-- returns nXn matrix (results from nXm * m*n multiplication ]

. Compute "eigenvectors" of matrix Sigma:
	-- "svd" and "eig" are two Octave functions that do this
		-- e.g., [U, S, V] = svd(Sigma)
	-- "SVD" = Singular Value Decomposition
	-- U matrix is also nXn -- columns are u vectors to u^m
	-- reduce to K dimensions by taking first K column vectors of U
	-- U_reduce is U with only the k dimensions you want -- so, nXk matrix
	-- Z = U_reduce' * X
		--  nXk' * nXm = 
			kXn * nXm = 
			kXm vector 

Reconstruction after data after compression
. X_approx = U_reduce * Z
	-- nXk * kXm =
	   nXm matrix

Choosing k (number of principal components)
. Average squared projection error / Total variation in the data should be < 0.01, so
	that 99% of variance is retained
	-- 95%, 90% may also be acceptable
. Average squared projection error:
	-- 1/m * sum ( || x^i - x_approx^i ||^2 )
. Total variation:
	-- 1/m * sum ( || x^i ||^2 )

. Algorithm:
	-- start with k=1, compute:
		U_reduce
		Z
		X_approx
	-- *** But this is inefficient ***
	-- S, as returned by 'svd' function, is a diagonal matrix
		-- you can take compute 1 - ( sum(S_ii) from 1-k ) / ( sum (S_ii) from 1-n )
			where 1-k is the first k rows of S
			denominator is all rows of S
		-- ii is the only non-zero value in that row of the S matrix
		-- if less than .01, or whatever your desired threshold, you're good

Advice for Applying PCA
. goal is to speed up your learning algorithm
. PCA defines a mapping from X (your original, uncompressed feature set) to Z, your
	reduced feature set
	-- this mapping should be devised *** only using your training set ***
	-- but should be applied to your cross validation and test sets as well
. when visualization is the goal:
	reduce k to 2 or 3, since you can't visualize more than that
. Don't use PCA to prevent overfitting
	-- Use regularization instead
	-- PCA is just taking into account X values, not considering y
. First try implementing your ML system _WITHOUT_ PCA
	-- only use it if you need it

