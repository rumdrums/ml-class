complex nonlinear hypotheses
. for certain types of problems, there are too many potential features
	for linear regression to be feasible. In order to be able to capture
	all the necessary quadratic features, ie squared and cubed terms, the number
	of features required increase (n^2/2) relative to n	
. example:
	-- number of pixels in effectively predicting whether an image is a car. if
	each image kept at 100x100 pixels, you would have 10,000 features, capturing all
	potential quadratic terms would be 50 million features.

neural networks
. old theory, goal is to mimic human brain
. recent resurgence due to cpu intensity, cloud makes constructing
	large neural networks feasible
. "one learning algorithm" hypothesis -- despite all the complexity of the brain,
	it's just one basic learning algorith that explains how entire thing learns
. neurons take inputs, process them, and send outputs
. x0 -- "bias unit"
. input: "dendrite"
. output: "axon"
. theta -- can be called weights
. neural network -- group of neurons strung together
	-- can be multiple layers in neural network
	-- input layer, hidden layer, output layer
. a_i^j -- refers to unit i in layer j
. Theta_j -- matrix of weights controlling function mapping from layer
	j to layer j+1
	-- so, if s_j units in layer j and s_(j+1) units in layer j+1, 
		Theta will be of dimension s_(j+1) x (sj + 1)
		-- example: layer 1: 3 units, layer 2: 3 units, Theta_j is 3x4 matrix
		-- example: layer 2: 2 units, layer 2: 4 units, Theta_j IS 4x3 matrix

forward propagation
. modelling effects of nodes throughout layers:
x1	a1\
x2->a2-->0-->h(x) 
x3	a3/

a_1^2 = g(z_1^2) -- where z_1^2 = Theta_10*x_0 + Theta_11*x_1 + Theta_12*x_2 + Theta_13*x_3
a_2^2 =  g(z_2^2) ...
a_3^2 = g(z_3^2) ...
h(x) = ... just like for the a layer....
		g(Theta_10*a_0^2 + Theta_11*a_1^2 + Theta_12*a_2^2 + Theta_13*a_3^2)

x = [x0; x1; x2; x3]
z^2 = [z_1^2; z_2^2; z_3^2]

. simplifying:
	-- x can also be thought of as a^1, ie, the inputs to second layer, a^2
	-- z^2=Theta^1*x -- or Theta^1*a^1
	-- a^2 = g(z^2) -- 3 dim. vector
	-- for the coefficient, add a_0^2 = 1 -- just like in previous stuff
	-- h(x) = a^3 = g(z^3)


