Anomaly Detection
. similar to both supervised and unsupervised

. given m examples, is x_test anomalous?
. eg, is user on a website unusual 
. is a particular system unusual?
	given load average
	memory use
	disk writes
	cpu load/network traffic
	-- if p(x) < e is small, may be anomaly

. Gaussian distribution
	x ~ N(u,sigma^2) ~ means "distributed as",
		x distributed normally with mean u and sd = sigma (variance = sigma^2)

	-- u is just 1/m*sum(x^i)
	-- sigma^2 = 1/m * sum(x^i-u)^2
	***	-- machine learners usually use 1/m, instead of 1/m-1
. bell curve formual
	= (1/sqrt(2pi)*sigma)^(-(x-u)^2/(2*sigma^2))

. Pi = Product notation, similar to sigma for summation but each element is multiplied
	by one another
. Given vector x with n dimensions:
	p(x) = p(x1;u1,sigma^2_1)*p(x2;u2,sigma^2_2) ... * p(x_n;u_n,sigma^2_n)
		 = Pi(x_j; u_j, sigma^2_j)
	-- assumes that each x_j is normally distributed and has its own values
		for u and sigma^2
	-- also assumes (loosely) that each x_j is independent

Anamoly detection algorithm
1) Choose features you think might be indicative of anomalous examples
2) Given {x^1,...,x^m}, fit parameters:
	u_j = 1/m * Sum(x^i_j) 
	sigma^2_j = 1/m * Sum(x^i_j - u_j)^2
3) Given new example x, compute p(x):
	p(x) = Pi(p(x_j; u_j, sigma^2_j)
         = Pi((1/sqrt(2pi)*sigma)^(-(x_j-u_j)^2/(2*sigma^2)))
	if p(x) is small, flag as anomaly

Real-number evaluation
. making decisions about what variables to include easier
	if we have way of evaluating algorithm
. Assume we have labeled data, where y = 0/1 if not / anomalous
	-- 10,000 normal examples, training set should be NOT anomalous examples
	-- 20-50 anamlous examples
. taking 10000 normal examples:
	-- training set: 6000
. put your anamlous ones into CV and test with rest of the normals:
	-- cross-validation: 2000 normal, 10 anomalous
	-- test: 2000 good, 10 anomalous
. steps for evaluation:
	1) fit model to training set
	2) on test / CSV sets, predict whether y = 1 or 0
. evaluation metrics:
	classification accuracy is not good -- because y is usually 0
		true positive / false positive / false negative / true negative
		precision / recall
		f1-score
		can also use CV set to choose parameter e
	
Anomaly Detection vs Supervised Learning
. Anomaly detection for:
	-- very small number of positive examples
	-- large number of negative examples
	-- many different "types" of anomalies, such that you
		*** don't expect to be able to predict from your few positive
		examples what future anomalies will look like ***, but rather
		just that something has gone wrong
	-- fraud detection, things that go wrong in quality control,
		eg aircraft engine anomaly, monitoring machines in
		data center
. Supervised learning
	-- large number of both positive and negative examples
	-- enough positive examples to be able to predict specific
		types of future positive outcomes
	-- eg, although difficult to detect what future spam may look like,
		we have enough examples of it to use supervised learning
	-- spam, weather, cancer classification

Choosing what Features to Use
. non-Gaussian features -- try to transform to make it Gaussian, although often
	non-Gaussian features will still look fine
		-- try to take log of feature
		-- or log(x+c) where c is a constant
		-- or square it, or take square root
		-- use histogram to analyze the results
. error analysis
	. p(x) should be large for normal examples, small for anomalous examples
	. Look at your anomalous examples and see if investigating them gives
		you ideas for what features to add
	. Choose features that might take on unusually large or small values
		in the event of an anomaly
	. Interactions between features -- eg, if software glitch, you may create a 
		feature that is cpu load^2 / network traffic

Multivariate Gaussian Distribution
. doesn't model p(x1),p(x2), ..., etc., separately
. rather, models p(x) all at once
. p(x;u,Sigma) [ Sigma is a covariance matrix ]
	= (1/(2pi^(n/2)*|sigma|^(1/2))) ^ (-1/2(x-u)'*Sigma^-1(x-u))
		|Sigma| means 'determinant' of Sigma -- det(Sigma) in Octave
. "smaller" sigma leads to narrower density plots, "larger" sigma leads to wider plots

Anomaly Detection using Multivariate Guassian Distribution
. u is n-dimensional vector
	-- u is 1/m*sum(x^i) -- (average)
. Sigma is nXn matrix
	-- Sigma is 1/m*sum(x^i -u)(x^i-u)' -- same as PCA
. Fit the model by setting u and Sigma
. Given a new example x, compute p(x)
	(1/(2pi^(n/2)*|sigma|^(1/2))) ^ (-1/2(x-u)'*Sigma^-1(x-u))
. Flag anomaly if p(x) < e
. Relationship to Original model:
	-- multivariate and original are identical, with constraint that your off-diagonal
		values of Sigma must be zero, eg:
			[ 1 0 0 ; 0 1 0; 0 0 1 ]
	-- which effectively means you can't model the correlations between different
		x parameters
	-- contours must be "axis-aligned", ie you can't have the slanted contours that
		multivariate model allows
Original Model vs Multivariate Model
. Original:
	-- original used more often
	-- have to manually create special features
	-- computationally cheaper
	-- ok even if m is small
. Multivariate:
	-- multivariate has advantage of capturing correlations
	-- don't have to create the special interactive features
	-- computationally more expensive, eg taking inverse of Sigma is expensive
	-- must have m > n, else Sigma is non-invertible
		-- ideally m >= 10*n
		-- can also be non-invertible if x1=x2, x3 = x4+x5, etc. (linearly dependent)
